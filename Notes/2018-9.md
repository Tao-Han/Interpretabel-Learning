
# Detailed Notes in 2018-9

1. [Feb 2016 : "Why Should I Trust You?": Explaining the Predictions of Any Classifier (KDD2016)](https://arxiv.org/abs/1602.04938) 

- **Intuition**

  - Explanation components (semantic components as features)
  - Local fidelity (locally loyal to the model being interpreted)
  - Limited complexity (rule/feature numbers, tree depth)
  - Model interpretability can be achieved by a set of representative instances
- **Model**
  - instance explanation **LIME**: Sparse Linear Model similar to Lasso
  - model explanation **LIME-SP** : Search the max cover of the explanation instance with submodular optimization
- **Evaluation**
  - simulated metrics
  - human experiments

---

2. [Jun 2017: Methods for Interpreting and Understanding Deep Neural Networks(DSP2017)](http://cn.arxiv.org/abs/1706.07979)

- **Tips**
  - This paper is based on a tutorial given at ICASSP 2017.
  - It introduces some recently proposed techniques of interpretation, like a **survey**

- **Definition and Distinction**

  - Interpretation: the mapping of an abstract concept into an understandable domain for human
  - Explanation: the collection of features of the interpretable domain

- **Opinions**

  - The learned concept tends to be represented by **a neuron in the top-layer**

  - Build a interpretable prototype in the input domain to represent the abstract learned concept.

  - The building of prototype can be formulated with **Activation Maximization(AM)** framwork.

    [Visualizing higher-layer features of a deep network(ICML2009)](http://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf)

    [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](http://cn.arxiv.org/abs/1312.6034)

- **Contents**

  - **Explain Model**
    - **AM**: Search for an input pattern to trigger a maximum response for a target function.
    - **AM + Expert**: Improve the AM framework with a density model called expert
    - **Approximation**: Sample from generative model to approximate complex density function.
    - **Global and Local Analysis**: different prototypes for multimodality
    - ...
  - **Explain Decision**
    - **Sensitivity Analysis**: The gradient at the data point answers the more/less question
    - **Layer-wise Relevance Propagation(LRP)**: relevance propogation based on NN architecture
    - **Taylor Decomposition**: Linearly approximate the decision function near the data point
    - ...
  - **Evaluation**
    - Explanation **Continuity**: Expalnations for similar data points are similar
    - Explanation **Selectivity**: Explanations redistribute relevance to variables with **strongest impact on the function** (pixel-flipping ...)

- *I would like to share the details of some techs above in the future*

---

3. [Oct 2017 : Interpretable Convolutional Neural Networks (CVPR2018)](https://arxiv.org/abs/1710.00935) 

- **Motivation**

  - Filters of CNN in **high conv-layer** tend to represent **complex semantic meanings**
  - Add a loss on these filters for **discrimitive representations** between categories
  - Filters capturing the object semantic are interpretations of the model

- **Model**

  - Design mask templates to capture the object in the feature map (the most actived)
  - The loss is in the form of mutual information between feature map and template
  - Apply approximation for computation convinence

- **Evaluation (All proxy functions)**

  - Part interpretability

    Compute the intersection-over-union score between receptive field of top-k activations on images and ground truth mask of images.

    [Network dissection: Quantifying interpretability of deep visual representations(CVPR2017)](https://arxiv.org/abs/1704.05796)

  - Location instability

    Computed the deviation of the distance between the inferred position and a specific ground-truth landmark among different images

    [Interpreting cnn knowledge via an explanatory graph(AAAI 2018)](https://arxiv.org/abs/1708.01785)

---

4. [On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation(PLOSONE2015)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) 

- **Motivation**
  - Compute **the heat map of contribution** of each pixel to the image classifier's decision
  - The existed work on **sensitivity analysis** may point far away from the decision boundary
  - The decision of classifiers can be decomposed pixel-wisely, satisifying a set of constriants
    - The complex decision function can be approximated by Talyor Decomposition linearly.
    - Multi-layer NN can be decomposed by layer-wise relevance propogation.
- **Model**
  - **Taylor-type decomposition**: linearly approximate the local function **near a neutral data**
  - **Layer-wise relevance propagation**: distributes class relevance layer-wisely by a propagation rule
- **Evaluation**
  - Pixel flipping method: discriminate between two heat maps
  - **A convincing evaluation of the quality of heat map remains uncovered**

---

5. [Dec 2013 : Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](http://cn.arxiv.org/pdf/1312.6034v2)

- **Motivation**
  - The prototype triggering the maximum score is representative of the class
  - Visualisation of the deep CNN provides a interpretation of the model and its prediction

- **Model**
  - **Class Model Visualisation**: generate an image to represent the class of CNN scoring model through back-propagation.
  - **Image Saliency Visualisation**: given an image and class, compute the saliency map of image on this class.
  - The saliency map can help initialise the **object segmentation** task.
- **Evaluation**
  - Visualisation result without **quantitative** experiment


# Detailed Notes in 2018-10

1. [Jul 2018: Supervised Local Modeling for Interpretability (NIPS2018)](http://cn.arxiv.org/abs/1807.02910)

- **Motivation**
  - **Example-based**, local and **global** explanations are the three most common types of explanation system, having their own strengthes and weaknesses in different contexts.
  - **SLIM** is a supervised local neighborhood approach assigning weights to training points called local training distribution.
  - SLIM combined the idea of sparse linear model and ensemble trees, holding properties including **high accuracy**, **global pattern detection** and **examplar explanation**. 
- **Model**
  - Using random forests for supervised neighborhood selection for local linear model (**SILO**) and doing feature selection via the **DStump**.
  - Neighborhood is defined as instances in the same leaf node in forest.(**SILO**)
  - Feature importance is defined as the reduction of impurity of labels when the feature get split on the root of the trees.(**DStump**)

- **Explanation**
  - Explain a prediction: the coefficients of local linear model reflect the local effect of each feature.
  - Diagnose global patterns from local explanation: visualize the distribution of each feature or do grid search. (Not very detailed)
  - Examplar exlanation

- **Evaluation**
  - High accuracy avoids the trade-off between interpretation and performance
  - Faithful self-explanation
  - Accurate black-box predictive model
  - Local distribution for global pattern detection

***Local linear model + Neighborhood sampling + LASSO for feature selection = LIME;***

***Local linear model +  RF for neighborhood selection + RF for feature selection = SLIM***

---



# Detailed Notes in 2018-9

1. [Feb 2016 : "Why Should I Trust You?": Explaining the Predictions of Any Classifier (KDD2016)](https://arxiv.org/abs/1602.04938) 

- Intuition
  - Explanation components (semantic components as features)
  - Local fidelity (locally loyal to the model being interpreted)
  - Limited complexity (rule/feature numbers, tree depth)
  - Model interpretability can be achieved by a set of representative instances
- Model
  - instance explanation LIME: Sparse Linear Model similar to Lasso
  - model explanation LIME-SP : Search the max cover of the explanation instance with submodular optimization
- Evaluation
  - simulated metrics
  - human experiments

2. [Jun 2017: Methods for Interpreting and Understanding Deep Neural Networks(DSP)](http://cn.arxiv.org/abs/1706.07979)

- Definition and Distinction

  - Interpretation: the mapping of an abstract concept into an understandable domain for human
  - Explanation: the collection of features of the interpretable domain

- Movtivation

  - The learned concept tends to be represented by a neuron in the top-layer

  - Build a prototype in the input domain (images, texts) that is interpretable and representative of the anstract learned concept.

  - The building of prototype can be formulated with Activation Maximization framwork.

    [Visualizing higher-layer features of a deep network(ICML2009)](http://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf)

    [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](http://cn.arxiv.org/abs/1312.6034)

- Model

- Evaluation

3. [Oct 2017 : Interpretable Convolutional Neural Networks (CVPR2018)](https://arxiv.org/abs/1710.00935) 

- Movtivation

  - Filters of CNN in high conv-layer tend to represent complex semantic meanings
  - Add a loss on these filters for discrimitive representations between categories
  - Filters capturing the object semantic are interpretations of the model

- Model

  - Design mask templates to capture the object in the feature map (the most actived)
  - The loss is in the form of mutual information between feature map and template
  - Apply approximation for computation convinence

- Evaluation (All proxy metric)

  - Part interpretability

    Compute the intersection-over-union score between receptive field of top-k activations on images and ground truth mask of images.

    [Network dissection: Quantifying interpretability of deep visual representations(CVPR2017)](https://arxiv.org/abs/1704.05796)

  - Location instability

    Computed the deviation of the distance between the inferred position and a specific ground-truth landmark among different images

    [Interpreting cnn knowledge via an explanatory graph(AAAI 2018)](https://arxiv.org/abs/1708.01785)

